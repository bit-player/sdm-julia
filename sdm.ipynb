{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SPARSE DISTRIBUTED MEMORY\n",
    "\n",
    "I wrote these programs to explore Pentti Kanerva's idea of Sparse Distributed Memory, introduced\n",
    "in his 1988 book of that title. The software accompanies the article [\"The Mind Wanders,\"](http://bit-player.org/2018/the-mind-wanders) published in July 2018 on [bit-player.org](http://bit-player.org).\n",
    "\n",
    "Copyright 2018 Brian Hayes\n",
    "\n",
    "Released under the [MIT LICENSE](https://opensource.org/licenses/MIT).\n",
    "\n",
    "The code was written for [Julia](https://julialang.org/) version 0.6.3. Versions 0.7 and 1.0 have now been released. They differ significantly from the earlier code, so that building a dual-use file is not practical. If you're running Julia 0.7 or later, please use sdmj7.ipynb.\n",
    "\n",
    "Disclaimer: I am a student of Julia, not a master of it. Suggestions and corrections welcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING STARTED\n",
    "\n",
    "You'll need a working version of the [Julia](https://julialang.org/) programming language, version 0.6, and the [Jupyter](http://jupyter.org/) environment for exploratory programming. And you need to get Julia and Jupyter talking to each other, which entails installing the IJulia package in Julia.\n",
    "\n",
    "Once you have Julia running in a Jupyter notebook, the next step is to install a couple of needed packages. Here's the incantation:\n",
    "\n",
    "```\n",
    "Pkg.add(DataStructures, StatsBase)\n",
    "```\n",
    "\n",
    "Now open this file in Jupyter, and evaluate all the cells (_Cell_ menu in Jupyter notebook, _Run_ menu in JupyterLab). \n",
    "\n",
    "In a fresh notebook cell (or in a console in JupyterLab), evaluate:\n",
    "\n",
    "```\n",
    "init_SDM()\n",
    "fill_memory(10000)\n",
    "```\n",
    "You may see some \"Info\" messages, but there should not be any Warnings or Errors. On my laptop in Julia 0.6.3, creating the 10,000 entries in the memory takes almost an hour. If you're in a hurry, `fill_memory(1000)` produces a usable system in one-tenth the time, although parameters such as the critical radius for recall will have different values.\n",
    "\n",
    "Now you're ready to play with the sparse distributed memory. See the section below \"Experiments with the SDM\" for some ideas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### IMPLEMENTATION NOTES\n",
    "\n",
    "Sparse distributed memory is all about computing with high-dimensional binary\n",
    "vectors -- long sequences of 0s and 1s interpreted as the coordinates of the\n",
    "vertices of a hypercube in a high-dimensional space. In the examples considered\n",
    "here the length of the vectors is generally 1,000 bits, and so there are $2^{1000}$ \n",
    "possible vectors, or patterns.\n",
    "\n",
    "The data structure I have chosen for representing these patterns is the Julia `BitVector`, an\n",
    "array of single bits, packed into 64-bit words. The standard Julia input and\n",
    "output routines interpret these bits as Boolean values (`false` and `true`), but I\n",
    "promise it's safe to think of them as 0s and 1s. A big advantage of using\n",
    "`BitVectors` is that primitive bitwise operations are executed in parallel on\n",
    "all the bits of a machine word. On a 64-bit processor,\n",
    "`xor`-ing two 1,000-bit vectors requires only 16 operations instead of 1,000.\n",
    "(As it happens, `xor` is the operation we're most concerned with.)\n",
    "\n",
    "A few alternatives to `BitVector`s are worth considering. My first thought when starting the project was to\n",
    "use `BigInt`s -- arbitrary-length integers. Here too we get the 64-fold\n",
    "speedup for bitwise operations such as `xor`. But some necessary operations\n",
    "are difficult to implement on `BigInt`s, such as the majority-rule algorithm for bit-by-bit\n",
    "pooling of vectors. _(See below under `registers`.)_ And converting `BigInt`s to other formats is costly. \n",
    "\n",
    "Another possible data structure for the binary patterns is an array of integers \n",
    "with values of +1 and -1, rather than 1 and 0. This choice would simplify the \n",
    "majority-rule operation. However, we lose the ability to operate on 64-bit chunks; \n",
    "instead we have to loop over the array elements one by one. This cost makes the overall program\n",
    "considerably slower. But we'll be using the +1/-1 scheme anyway for a slightly different purpose. \n",
    "_(Again, see `registers` below.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PACKAGES, IMPORTS, AND OVERRIDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using StatsBase.sample  # for sampling without replacement\n",
    "using JLD               # storing and loading Julia-flavored HDF5 files\n",
    "using DataStructures    # for accumulator/counter in neighbor distributions\n",
    "\n",
    "\n",
    "# DEV TOOLS  -- uncomment if needed\n",
    "\n",
    "# using BenchmarkTools    # may also require the Compat package\n",
    "# using Base.Profile      # comment out when not needed\n",
    "\n",
    "\n",
    "### DISPLAY OF BITVECTORS\n",
    "\n",
    "# The default display of BitVectors is a list of true and false values,\n",
    "# which wastes space and is nearly incomprehensible for 1,000-bit vectors.\n",
    "# Here we pick up a routine called `bitshow` from the Julia source code. \n",
    "# When we override the built-in \"show\" and \"print\" methods, BitVectors \n",
    "# appear as strings of 0s and 1s grouped into 8-bit bytes and 64-bit words.\n",
    "\n",
    "import Base.show      # so that we can modify display of BitVectors\n",
    "import Base.print     # ditto\n",
    "\n",
    "show(x::BitVector) = Base.bitshow(x)\n",
    "print(x::BitVector) = Base.bitshow(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum of n choose k for k in 1:m. Needed to calculate the \n",
    "# number of vectors inside a given Hamming radius.\n",
    "\n",
    "function cum_binomial(n, m)\n",
    "    sum = zero(BigInt)\n",
    "    for k in 1:m\n",
    "        sum = binomial(BigInt(n), k)\n",
    "    end\n",
    "    return sum\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicate. Applied to any iterable (such as an array), returns\n",
    "# `true` iff all elements of the iterable are `==`. (Seems like\n",
    "# there ought to be an easier way... )\n",
    "\n",
    "function allequal(iterable)\n",
    "    length(iterable) < 2 && return true\n",
    "    val = iterable[1]\n",
    "    all(x->x==val, iterable)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A parameters turn up repeatedly in the SDM model, and it's important they\n",
    "# they have consistent values throughout. In particular, all of the pattern\n",
    "# vectors must have the same dimensionality d. To ensure consistency I have\n",
    "# made d a global constant, and I've treated N (the number of hard locations)\n",
    "# and r (the radius of the access circle) in the same way.\n",
    "\n",
    "# The constant SDM holds a pointer to the memory itself, an array of structures\n",
    "# that represent Kanerva's hard locations. IDX is an index to the items stored\n",
    "# memory; for more about this components see the section on \"Labels and the IDX.\"\n",
    "\n",
    "# A further note on Julia constants: They are not really constant. The compiler \n",
    "# will not stop you from assigning a new value to d, as in d = 10000, although \n",
    "# it will issue a warning message. The only total no-no is trying to change the\n",
    "# type of d, as in d = 1000.0. However, even though you can change the value of\n",
    "# a constant, you can't just treat it as a typed variable. Functions compiled\n",
    "# when the old value was in effect will likely have that old value baked in. You'll\n",
    "# need to restart the Julia system to have the new value recognized.\n",
    "\n",
    "# So why not just make the parameters global variables rather than constants? \n",
    "# Efficiency. Julia's type inference system can't trust anything about a global\n",
    "# variable; its type could change from moment to moment, and so it has to be\n",
    "# checked at runtime.\n",
    "\n",
    "\n",
    "\n",
    "const N = 10^6    # number of hard locations in the sparse distributed memory\n",
    "const d = 1000    # dimensions of vectors and registers\n",
    "const r = 451     # radius of the access circle for sparse distributed memory\n",
    "\n",
    "struct Loc                      # each hard location has an address and\n",
    "    address::BitVector          # a register for pooled content\n",
    "    register::Vector{Int8}\n",
    "end\n",
    "\n",
    "const SDM = Vector{Loc}(N)      # the N-element vector of hard locations\n",
    "\n",
    "const IDX = Dict{String, BitVector}()    # for keeping track of what we've stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY VECTORS\n",
    "\n",
    "The basic unit of information in the SDM is a $d$-dimensional vector of bits. Mostly,\n",
    "we want _random_ vectors. Randomness assures that all the vectors will,\n",
    "with high probability, be widely separated in the $d$-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKING BITVECTORS\n",
    "\n",
    "make_rand_bitvec() = bitrand(d)\n",
    "\n",
    "make_zeros_bitvec() = falses(d)\n",
    "\n",
    "make_ones_bitvec() = trues(d)\n",
    "\n",
    "\n",
    "# The functions above create fresh vectors suitable for modification.\n",
    "# If we're just using a vector as a target of comparison with some other\n",
    "# vector, we can save outselves the trouble of building a fresh copy\n",
    "# and just use a constant.\n",
    "\n",
    "const zeros_bitvec = falses(d)\n",
    "const ones_bitvec  = trues(d);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISTANCE BETWEEN BITVECTORS\n",
    "\n",
    "# We measure the distance between two vectors in terms of Hamming distance,\n",
    "# or in other words by the Manhattan metric. For binary vectors this is just\n",
    "# the number of bit positions at which two vectors differ. We can calculate\n",
    "# this value by xor-ing the two vectors and counting the 1s in the result.\n",
    "# (Note that the dot in \"xor.(u, v)\" signifies that we are broadcasting the\n",
    "# xor operation elementwise across the two vectors.)\n",
    "\n",
    "# An important observation is that for large d the distance between any two\n",
    "# random d-bit vectors will almost surely be close to d/2.\n",
    "\n",
    "hamming_distance(u::BitVector, v::BitVector) = count(xor.(u, v))\n",
    "\n",
    "# The only other meaningful form of comparison between binary patterns in the SDM is\n",
    "# simple equality, which can be checked by the standard operator `==`. There's\n",
    "# no sense of ranking or comparing magnitudes. No BitVector is greater than or\n",
    "# less than another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTICS OF NEIGHBOR DISTANCES\n",
    "\n",
    "# Generate 'reps' pairs of random vectors, and build a histogram of the Hamming\n",
    "# distances between them.\n",
    "\n",
    "function distance_histogram(reps)\n",
    "    ct = counter(Int64)\n",
    "    for i = 1:reps\n",
    "        dist = hamming_distance(make_rand_bitvec(), make_rand_bitvec())\n",
    "        push!(ct, dist)\n",
    "    end\n",
    "    return sort(collect(ct), by = tuple -> first(tuple))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARSE PRINTED REPRESENTATION OF BITVECTOR\n",
    "\n",
    "# Sometimes you might want to take the output of show() or print()\n",
    "# and feed it back into the program.\n",
    "\n",
    "function parse_vector_rep(s::String)\n",
    "    v = BitVector()\n",
    "    for c in s\n",
    "        if c == '1'\n",
    "            push!(v, true)\n",
    "        elseif c == '0'\n",
    "            push!(v, false)\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGISTERS\n",
    "\n",
    "Apart from the binary vectors, there's another essential data structure, which I'm calling \n",
    "a `register`. It too is a$d$-dimensional vector, but the components are not individual bits \n",
    "but rather 8-bit signed integers (Julia type `Int8`). In the SDM, a register holds the result \n",
    "of \"pooling\" multiple `BitVectors`, a process akin to summation or averaging.\n",
    "\n",
    "Note: An `Int8` has a range of `[-128, +127]`, which means that pooling more than\n",
    "127 vectors could cause an overflow. Overflow in Julia is particularly pernicious, because\n",
    "it is treated as _rollover_: `Int8(127) + Int8(1) = -128`. This risk was acceptable in\n",
    "my experiments because I was never pooling more than about a dozen vectors. If the risk\n",
    "of rollover worries you, you could change the type to `Int16` at the cost of another\n",
    "gigabyte of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A register is an array of d Int8s, all initialized to zero. \n",
    "\n",
    "make_zero_register() = zeros(Int8, d)\n",
    "\n",
    "\n",
    "# Inject vector v into the pooled contents of register reg. For each 1 bit in\n",
    "# v the corresponding element of reg is incremented; for each 0 bit the reg\n",
    "# element is decremented. Thus at any moment the count stored in a register element\n",
    "# will represent the number of 1s minus the number of 0s received. Note\n",
    "# that the reg argument is altered, and no value is returned.\n",
    "\n",
    "function write_to_register!(reg, v::BitVector)\n",
    "    (length(reg) == length(v)) || throw(DimensionMismatch(\"Register and vector must have the same length.\"))\n",
    "    for i in 1:length(v)\n",
    "        @inbounds reg[i] += v[i] ? one(Int8) : -one(Int8)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Now for the inverse of write-to-register!. When reading\n",
    "# from a register, we need to convert the contents back into a BitVector\n",
    "# representation. Specifically, we take an array of Ints, and convert each\n",
    "# element of the array into a binary digit. A positive Int becomes a 1, a\n",
    "# negative Int becomes a 0; a 0 Int is randomly assigned a value of either 1 or 0.\n",
    "\n",
    "function read_from_register(reg)\n",
    "    d = length(reg)\n",
    "    v = make_zeros_bitvec()\n",
    "    @inbounds for i in 1:d\n",
    "        if reg[i] > 0\n",
    "            v[i] = true\n",
    "        elseif reg[i] == 0\n",
    "            v[i] = rand(Bool)\n",
    "        end\n",
    "    end\n",
    "    v\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA STRUCTURES FOR SPARSE DISTRIBUTED MEMORY\n",
    "\n",
    "So far we've constructed the basic components of an SDM: high-dimensional bit vectors, and registers for pooling them. Now we need a way to store large numbers of vectors, and thereafter retrieve them.\n",
    "\n",
    "Providing space for all $2^{1000}$ possible vectors is out of the question. In the memory scheme devised by Kanerva, storage is provided in a small, random subset of the total address space; these selected addresses are called _hard locations_. In the reference model there are $10^{6}$ hard locations. The data structure for a hard location gangs together a `BitVector` that serves as an address, and a `register` that will store everything written to that address, by pooling bits into `Int8`s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZE THE MEMORY\n",
    "\n",
    "# Set up a memory with N hard locations, each of which has an address vector\n",
    "# and a register of d elements. No value is returned; the created data structure\n",
    "# is bound to the global variable SDM. Running time is a few seconds.\n",
    "\n",
    "# This is still an empty memory; there is nothing stored in it.\n",
    "\n",
    "function init_SDM()\n",
    "    for i in 1:N\n",
    "        SDM[i] = Loc(make_rand_bitvec(), make_zero_register())\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STORING AND FETCHING\n",
    "\n",
    "# A conventional computer memory stores each data item at a single address, and\n",
    "# each such address can hold only one data item. The SDM is quite different. A data\n",
    "# item is written at many addresses -- perhaps 1,000 -- and each address holds \n",
    "# multiple items, all mixed together.\n",
    "\n",
    "\n",
    "# How to write a data vector `v` into the SDM: Scan the entire array of hard locations,\n",
    "# measuring the hamming distance between `v` and each location's address; whenever the\n",
    "# distance is less than or equal to the access radius `r`, write the vector into the location's\n",
    "# register. With a million hard locations and `r == 451`, there are typically about 1,000\n",
    "# locations inside the circle of radius `r`.\n",
    "\n",
    "function SDMstore(v::BitVector)\n",
    "    for loc in SDM\n",
    "        if hamming_distance(v, loc.address) <= r\n",
    "            write_to_register!(loc.register, v)\n",
    "         end\n",
    "     end\n",
    "end\n",
    "\n",
    "\n",
    "# Reading from the SDM is also a distributed process. When you ask for the content of\n",
    "# the memory at address `v`, the `SDMfetch` procedure searches for all hard locations\n",
    "# within `r` bits of `v`, and pools their contents. The value returned is computed\n",
    "# by a bit-by-bit majority-rule algorithm.\n",
    "\n",
    "# `SDMstore(v)` followed by `SDMfetch(v)` will return the value `v` with high probability\n",
    "# (near certainty if the memory is not filled beyond capacity). If a vector `u` has NOT\n",
    "# been stored in the memory, `SDMfetch(u)` returns an essentially random result.\n",
    "\n",
    "function SDMfetch(v::BitVector)\n",
    "    accum = zeros(Int64, length(v))\n",
    "    for loc in SDM\n",
    "        if hamming_distance(v, loc.address) <= r\n",
    "            accum += loc.register\n",
    "        end\n",
    "     end\n",
    "     return read_from_register(accum)\n",
    " end\n",
    "\n",
    "\n",
    "# SDMsearch attempts to retrieve a stored item `v` based on an approximate cue `u`. The\n",
    "# attempt is likely to succeed if the distance between `u` and `v` is not too great. (The\n",
    "# meaning of \"not too great\" depends on d, r, N, and the number of stored items M.) The\n",
    "# algorithm is simply a recursive application of SDMfetch, which in successful cases\n",
    "# will converge on a fixed point. The parameter `limit=30` controls how long to keep\n",
    "# trying. \n",
    "\n",
    "function SDMsearch(u::BitVector, limit=30)\n",
    "    prev = copy(u)\n",
    "    for i in 1:limit\n",
    "        next = SDMfetch(prev)\n",
    "        if next == prev\n",
    "            return next\n",
    "        else\n",
    "            prev = next\n",
    "        end\n",
    "    end\n",
    "    return zeros_bitvec    # sentinel indicating failure\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To test SDMsearch, we need to be able to displace a vector by a specified\n",
    "# distance, but in a random direction. Thus we have:\n",
    "#\n",
    "#    hamming_distance(v, displace(v, 200)) == 200\n",
    "\n",
    "function displace(v, distance)\n",
    "    w = copy(v)\n",
    "    targets = sample(collect(1:d), distance, replace=false, ordered=true)\n",
    "    for i in targets\n",
    "        w[i] = ~v[i]\n",
    "    end\n",
    "    return w\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABELS AND THE IDX\n",
    "\n",
    "The set of functions introduced so far amounts to a full memory system: You can store things in it and get them back again, even if you have only a partial address. As a model of biological memory, it could be considered complete. But it's also very hard for the human observer to follow what's going on. A long sequence of 1s and 0s might represent a remembered concept such as \"sage\" or \"butterfly,\" but on a computer screen such patterns are incomprehensible; we cannot associate them with their meanings. \n",
    "\n",
    "To deal with this problem, we can attach a label to each pattern. The labels are short character strings that are easier to recognize than long vectors. But keep in mind: The labels are metadata, outside the memory system itself. They could be removed entirely and nothing would change in the operation of the SDM.\n",
    "\n",
    "When a pattern is committed to memory, it goes into a dictionary, `IDX`, where the label is the key and the pattern is the associated value. The label is any valid character string. The stored binary vector is generated `make_rand_bitvec()`.\n",
    "\n",
    "Even apart from the labels, it's important to keep a list of everything that's been stored in the memory. Without it, there's no practical way of ever getting anything out again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To implement the labeling system, we add new methods to several functions. In most\n",
    "# cases the new methods accept a label in the place of a BitVector.\n",
    "\n",
    "\n",
    "# Create an IDX entry where `(key, value) == (label, v)`. If v is not supplied,\n",
    "# generate a random BitVector of length d. Finally, store v in the SDM. Note that\n",
    "# if `label` is already present in IDX, then any supplied value of v will be\n",
    "# ignore, and the existing vector from IDX will be stored instead. Thus a value\n",
    "# cannot be overwritten in IDX, but the same value can be stored multiple times\n",
    "# in SDM.\n",
    "\n",
    "function SDMstore(label::String, v::BitVector = make_rand_bitvec())\n",
    "    w = get!(IDX, label, v)\n",
    "    SDMstore(w)\n",
    "end\n",
    "\n",
    "\n",
    "# Retrieve an item from SDM based on its identifying label in IDX.\n",
    "\n",
    "function SDMfetch(label::String)\n",
    "    haskey(IDX, label) || error(\"Item $label not indexed.\")\n",
    "    SDMfetch(IDX[label])\n",
    "end\n",
    "\n",
    "\n",
    "# Do a recursive search of SDM for the vector associated with `label`.\n",
    "\n",
    "function SDMsearch(label::String, limit=30)\n",
    "    haskey(IDX, label) || error(\"Item $label not indexed.\")\n",
    "    SDMsearch(IDX[label], limit)\n",
    "end\n",
    "    \n",
    "\n",
    "# Inverse access to IDX: Given a stored vector, sequentially search IDX\n",
    "# and return the corresponding label. This allows for usage like so:\n",
    "#\n",
    "#      IDXlookup(SDMfetch(\"beluga\")) --> \"beluga\"\n",
    "\n",
    "function IDXlookup(v::BitVector)\n",
    "    for (label, vec) in IDX\n",
    "        if v == vec\n",
    "            return label\n",
    "        end\n",
    "    end\n",
    "    return \"\"\n",
    "end\n",
    "\n",
    "\n",
    "# Look up the vector associated with `label`, and return a new vector\n",
    "# displaced by `distance`.\n",
    "\n",
    "function displace(label::String, distance)\n",
    "    haskey(IDX, label) || error(\"Item $label not indexed.\")\n",
    "    displace(IDX[label], distance)\n",
    "end\n",
    "\n",
    "\n",
    "# Another convenience function: Calculate distance between vectors\n",
    "# given their labels.\n",
    "\n",
    "function hamming_distance(ulabel::String, vlabel::String)\n",
    "    haskey(IDX, ulabel) || error(\"Item $ulabel not indexed.\")\n",
    "    haskey(IDX, vlabel) || error(\"Item $vlabel not indexed.\")\n",
    "    hamming_distance(IDX[ulabel], IDX[vlabel])\n",
    "end\n",
    "\n",
    "\n",
    "# Meaningful labels are great, but I don't want to have to come up with\n",
    "# 10,000 of them in order to fill up the memory for testing purposes.\n",
    "# Instead, let's do it with random labels. (Julia's `randstring` draws\n",
    "# characters from an alphabet of size 62; the chance of a repeated\n",
    "# eight-character label is about $10^{-14}$)\n",
    "\n",
    "random_label(n=8) = randstring(n)\n",
    "\n",
    "\n",
    "# Add C randomly labeled random vectors to the IDX, and store them in \n",
    "# the SDM. This is a lengthy operation. On my laptop, `fill_memory(10000)`\n",
    "# takes almost an hour.\n",
    "\n",
    "function fill_memory(C)\n",
    "    for i in 1:C\n",
    "        SDMstore(random_label())\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Finally, we may need to clear the slate.\n",
    "\n",
    "function init_IDX()\n",
    "    for k in keys(IDX)\n",
    "        delete!(IDX, k)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING THE MODEL TO DISK\n",
    "\n",
    "The data structures for the SDM and the IDX are serialized and saved in the JLD file format, a Julia-adapted flavor of HDF5. For more on what's going on here, see [the Julia Data module](https://github.com/JuliaIO/JLD.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write it to disk...\n",
    "\n",
    "function save_memory(filename::String)\n",
    "    if length(filename) < 5 || filename[end-3:end] != \".jld\"     # ensure .jld extension\n",
    "        filename = filename * \".jld\"\n",
    "    end\n",
    "    save(filename, \"SDM\", SDM, \"IDX\", IDX)\n",
    "end\n",
    "\n",
    "\n",
    "# ... and read it back in.\n",
    "\n",
    "function load_memory(filename::String)\n",
    "    return (load(filename, \"SDM\"), load(filename, \"IDX\"))\n",
    "end\n",
    "\n",
    "\n",
    "# The file created will be about 2.7 gigabytes. Saving it takes roughly\n",
    "# 15 minutes, a little less to load it.\n",
    "\n",
    "\n",
    "\n",
    "# The `load_memory` function should be invoked in the following form:\n",
    "#\n",
    "# (SDM, IDX) = load_memory(filename)\n",
    "#\n",
    "# You will see two warnings noting the redefinition of the\n",
    "# constants SDM and IDX. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENTS WITH THE SDM\n",
    "\n",
    "We now have a sparse distributed memory stuffed with 10,000 randomly labeled random patterns. It's time to test retrieving those patterns, and then see what else it might be fun to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step through the entire index of 10,000 labels, fetch each of the corresponding\n",
    "# vectors from the SDM, and make sure it matches the one in the IDX. There should\n",
    "# be no failures reported.\n",
    "\n",
    "function test_exact_recall()\n",
    "    failures = []\n",
    "    failure_count = 0\n",
    "    for label in keys(IDX)\n",
    "        v = IDX[label]\n",
    "        w = SDMfetch(v)\n",
    "        if w != v\n",
    "            push!(failures, label)\n",
    "            failure_count += 1\n",
    "        end\n",
    "    end\n",
    "    return (failure_count, failures)\n",
    "end\n",
    "\n",
    "\n",
    "# Choose a stored vector at random, displace it in a random direction\n",
    "# by `distance`, and see whether this approximate probe retrieves the\n",
    "# correct v from the SDM. Repeat `reps` times. The outcome will depend on \n",
    "# the value of `distance`. Below the critical convergence radius (somewhere\n",
    "# between 200 and 210 for a memory with 10,000 stored elements), failures\n",
    "# should be few. Above that radius, successes will be few. In one experiment\n",
    "# at distance=200, I saw 43/100 failures.\n",
    "\n",
    "function test_approx_recall(distance, reps)\n",
    "    failures = []\n",
    "    failure_count = 0\n",
    "    labels = collect(keys(IDX))\n",
    "    for i in 1:reps\n",
    "        label = rand(labels)\n",
    "        v = IDX[label]\n",
    "        x = displace(v, distance)\n",
    "        w = SDMsearch(x)\n",
    "        if w != v\n",
    "            push!(failures, label)\n",
    "            failure_count += 1\n",
    "        end\n",
    "    end\n",
    "    return (failure_count, failures)\n",
    "end\n",
    "\n",
    "\n",
    "# A more intimate glimpse of the process of converging from an inexact probe.\n",
    "# For a converging case, the output looks like this:\n",
    "\n",
    "# start-to-target: 200\n",
    "# next-prev: 150,  next-start: 150,  next-target: 154\n",
    "# next-prev: 61,  next-start: 191,  next-target: 129\n",
    "# next-prev: 25,  next-start: 210,  next-target: 108\n",
    "# next-prev: 31,  next-start: 217,  next-target: 83\n",
    "# next-prev: 26,  next-start: 215,  next-target: 59\n",
    "# next-prev: 38,  next-start: 201,  next-target: 21\n",
    "# next-prev: 21,  next-start: 200,  next-target: 0\n",
    "# next-prev: 0,  next-start: 200,  next-target: 0\n",
    "\n",
    "function trace_recursive_fetch(label, distance, limit=30)\n",
    "    target = IDX[label]\n",
    "    start = displace(target, distance)\n",
    "    prev = start\n",
    "    println(\"start-to-target: $(hamming_distance(start, target))\")\n",
    "    for i in 1:limit\n",
    "        next = SDMfetch(prev)\n",
    "        np = hamming_distance(next, prev)\n",
    "        ns = hamming_distance(next, start)\n",
    "        nt = hamming_distance(next, target)\n",
    "        println(\"next-prev: $np,  next-start: $ns,  next-target: $nt\")\n",
    "        prev = next\n",
    "        np == 0 && break\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List successive values of hamming distance from probe to target\n",
    "# as the recursive fetching proceeds. E.g.,\n",
    "#\n",
    "# show(convergence_trajectory(200, 30)) --> [200, 177, 167, 149, 130, 113, 98, 82, 60, 30, 2, 0]\n",
    "    \n",
    "function convergence_trajectory(distance, limit)\n",
    "    target = rand(collect(values(IDX)))\n",
    "    probe = displace(target, distance)\n",
    "    trajectory = Array{Int64, 1}()\n",
    "    for i in 1:limit\n",
    "        s = hamming_distance(probe, target)\n",
    "        push!(trajectory, s)\n",
    "        if s == 0\n",
    "            break\n",
    "        end\n",
    "        probe = SDMfetch(probe)\n",
    "    end\n",
    "    return trajectory\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tabulate the number of vectors that converge throughout\n",
    "# a range of displacement values. At each displacement in the range,\n",
    "# test `reps` randomly chosen vectors.\n",
    "\n",
    "function convergence_stats(lo, step, hi, reps)\n",
    "    target = rand(collect(values(IDX)))\n",
    "    counts = zeros(Int64, hi)\n",
    "    for i in lo:step:hi\n",
    "        for j in 1:reps\n",
    "            probe = displace(target, i)\n",
    "            distance = i\n",
    "            limit = 30\n",
    "            while distance > 0 && limit > 0\n",
    "                probe = SDMfetch(probe)\n",
    "                distance = hamming_distance(probe, target)\n",
    "                limit -= 1\n",
    "            end\n",
    "            if distance == 0\n",
    "                counts[i] += 1\n",
    "            end\n",
    "        end\n",
    "        @printf(\"%d  %d\\n\", i, counts[i])\n",
    "    end\n",
    "    return counts\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run `reps` iterations of SDMsearch, all displaced from their\n",
    "# target by a fixed `distance`, and calculate the fraction that\n",
    "# converge to the target. \n",
    "\n",
    "function convergence_fraction(distance, reps)\n",
    "    vs = collect(values(IDX))\n",
    "    hits = 0\n",
    "    for i in 1:reps\n",
    "        v = displace(rand(vs), distance)\n",
    "        if SDMsearch(v) != zeros_bitvec\n",
    "            hits += 1\n",
    "        end\n",
    "    end\n",
    "    return hits / reps\n",
    "end\n",
    "        \n",
    "\n",
    "# A version with a specific target vector, rather than a randomly\n",
    "# chosen one.\n",
    "\n",
    "function convergence_fraction(label::String, distance, reps)\n",
    "    u = IDX[label]\n",
    "    hits = 0\n",
    "    for i in 1:reps\n",
    "        v = displace(u, distance)\n",
    "        if SDMsearch(v) != zeros_bitvec\n",
    "            hits += 1\n",
    "        end\n",
    "    end\n",
    "    return hits / reps\n",
    "end\n",
    "        \n",
    "    \n",
    "# Apply a bisection method to find the convergence radius -- the\n",
    "# displacement where half the trials converge and half don't.\n",
    "\n",
    "function estimate_convergence_radius(lo, hi, reps)\n",
    "    hi - lo <= 1 && return (lo, hi)   \n",
    "    mid = div(lo + hi, 2)\n",
    "    frac = convergence_fraction(mid, reps)\n",
    "    println(\"At distance $mid convergence fraction is $frac\")\n",
    "    if frac < 0.5\n",
    "        estimate_convergence_radius(lo, mid, reps)\n",
    "    else\n",
    "        estimate_convergence_radius(mid, hi, reps)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# So far, we have been detecting convergence by detecting the presence of\n",
    "# a fixed point, where SDMfetch(x) = x. But how do we know that the fixed\n",
    "# point is in fact the target vector we were trying to recall? This procedure\n",
    "# checks for the possibility of reach other fixed points.\n",
    "\n",
    "function verify_convergence_destination(ulabel::String, displacement, reps, limit=30)\n",
    "    converge_count = 0\n",
    "    failure_count = 0\n",
    "    wrong_target_count = 0\n",
    "    for i in 1:reps\n",
    "        u = IDX[ulabel]\n",
    "        v = displace(ulabel, displacement)\n",
    "        w = SDMsearch(v, limit)\n",
    "        if w == zeros_bitvec\n",
    "            failure_count += 1\n",
    "        elseif u == w\n",
    "            converge_count += 1\n",
    "        else\n",
    "            wrong_target_count += 1\n",
    "            wlabel = IDXlookup(w)\n",
    "            if wlabel == \"\"\n",
    "                wlabel = \"Unstored vector\"\n",
    "            end\n",
    "            uw = hamming_distance(u, w)\n",
    "            uv = hamming_distance(u, v)\n",
    "            vw = hamming_distance(v, w)\n",
    "            println(\"$i: displace($ulabel, $displacement) --> $wlabel.\")\n",
    "            println(\"$ulabel = u = \")\n",
    "            show(u)\n",
    "            println()\n",
    "            println(\"displace($ulabel, $displacement) = v = \")\n",
    "            show(v)\n",
    "            println()\n",
    "            println(\"$wlabel = w = \")\n",
    "            show(w)\n",
    "            println()\n",
    "            println(\"uv dist = $uv,  vw dist = $vw,  uw dist = $uw\")\n",
    "        end\n",
    "    end\n",
    "    println(\"\"\"Converged to correct target: $converge_count\n",
    "        Failed to converge: $failure_count\n",
    "        Converged to wrong target: $wrong_target_count\"\"\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEAREST NEIGHBORS\n",
    "\n",
    "# Given a vector label, find its nearest neighbor among all\n",
    "# stored patterns, and report the hamming distance.\n",
    "\n",
    "function nearest_neighbor(u::BitVector)\n",
    "    best_label = \"\"\n",
    "    best_distance = d\n",
    "    for (l, v) in IDX\n",
    "        dist = hamming_distance(u, v)\n",
    "        if dist < best_distance\n",
    "            best_label = l\n",
    "            best_distance = dist\n",
    "        end\n",
    "    end\n",
    "    return (best_label, best_distance)\n",
    "end\n",
    "\n",
    "\n",
    "function nearest_neighbor(label::String)\n",
    "    haskey(IDX, label) || error(\"Item $label not indexed.\")\n",
    "    best_label = \"\"\n",
    "    best_distance = d\n",
    "    u = IDX[label]\n",
    "    for (l, v) in IDX\n",
    "        l == label && continue\n",
    "        dist = hamming_distance(u, v)\n",
    "        if dist < best_distance\n",
    "            best_label = l\n",
    "            best_distance = dist\n",
    "        end\n",
    "    end\n",
    "    return (best_label, best_distance)\n",
    "end\n",
    "\n",
    "\n",
    "# Among all stored patterns, find the pair with the smallest\n",
    "# Hamming distance\n",
    "\n",
    "function nearest_neighbor_pair()\n",
    "    best_left = \"\"\n",
    "    best_right = \"\"\n",
    "    best_distance = d\n",
    "    ks = collect(keys(IDX))\n",
    "    vs = collect(values(IDX))\n",
    "    for i in 1:length(ks) - 1\n",
    "        for j in i+1:length(ks)\n",
    "            dist = hamming_distance(vs[i], vs[j])\n",
    "            if dist < best_distance\n",
    "                best_left = ks[i]\n",
    "                best_right = ks[j]\n",
    "                best_distance = dist\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return (best_left, best_right, best_distance)\n",
    "end\n",
    "\n",
    "\n",
    "# For any two vectors (whether of not they are actually stored\n",
    "# in the SDM), count the hard locations that would be common to\n",
    "# their two access circles.\n",
    "\n",
    "function hard_locations_in_common(u::BitVector, v::BitVector)\n",
    "    u_indices = Vector{Int64}()\n",
    "    v_indices = Vector{Int64}()\n",
    "    for i in 1:length(SDM)\n",
    "        if hamming_distance(u, SDM[i].address) <= r\n",
    "            push!(u_indices, i)\n",
    "        end\n",
    "        if hamming_distance(v, SDM[i].address) <= r\n",
    "            push!(v_indices, i)\n",
    "        end\n",
    "    end\n",
    "    uv_indices = intersect(u_indices, v_indices)\n",
    "    return (length(uv_indices), uv_indices)\n",
    "end\n",
    "\n",
    "\n",
    "# Probing the SDM with random vectors, count how many converge\n",
    "# on a stored vector, how many converge on some other fixed point,\n",
    "# and how many fail to converge within `limit` repetitions of the\n",
    "# recursive recall process.\n",
    "\n",
    "function fate_of_random_vectors(reps, limit=30)\n",
    "    stored_item = 0\n",
    "    other_fixed_point = 0\n",
    "    did_not_converge = 0\n",
    "    for i in 1:reps\n",
    "        probe = make_rand_bitvec()\n",
    "        fate = SDMsearch(probe, limit)\n",
    "        if fate == zeros_bitvec\n",
    "            did_not_converge += 1\n",
    "        elseif IDXlookup(fate) == \"\"\n",
    "            other_fixed_point += 1\n",
    "        else\n",
    "            stored_item += 1\n",
    "        end\n",
    "    end\n",
    "    println(\"\"\"\n",
    "        stored items: $stored_item\n",
    "        other fixed points: $other_fixed_point\n",
    "        did not converge: $did_not_converge\n",
    "    \"\"\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCEMENT THROUGH REPETITION\n",
    "\n",
    "A well-known feature of human memory is that rehearsal improves recall. Repetition is how we memorize a poem or a list of state capitals. Something analogous happens in the SDM. Storing the same vector twice makes it easier to recall: The radius-of-convergence increases, so that the target pattern is accessible from a larger volume of the 1,000-dimensional hypercube.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store the vector associated with `label` in the SDM `n` times.\n",
    "\n",
    "function SDMstore(label::String, n::Int)\n",
    "    for i in 1:n\n",
    "        SDMstore(label)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECIMATION AND RELIABILITY\n",
    "\n",
    "One of the sterling qualities of distributed memory is robustness, the ability to retain information even when elements of the hardware fail. The `decimateSDM` procedure allows for testing this property. (Try running `convergence_fraction` before and after wiping out half the hard locations in the memory.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zero out the content of a register, such as the register field\n",
    "# of a hard location.\n",
    "\n",
    "function reset_register!(reg)\n",
    "    for i in 1:length(reg)\n",
    "        reg[i] = 0\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Zero the content registers of k randomly chosen hard locations\n",
    "# in the SDM\n",
    "\n",
    "function decimateSDM(k)\n",
    "    target_indices = sample(collect(1:N), k, replace=false)\n",
    "    for t in target_indices\n",
    "        reset_register!(SDM[t].register)\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINDING AND BUNDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BINDING TWO VECTORS\n",
    "\n",
    "# The xor operation has another vital role as well: It projects a random\n",
    "# binary vector into another region of the d-dimensional space, preserving\n",
    "# distance relationships. That is, if hamming_distance(u, v) == s, then also\n",
    "# hamming_distance(xor.(u, c), xor.(v, c)) == s, when we project both u and v\n",
    "# by c. Kanerva calls this projection operator \"bind\".\n",
    "\n",
    "# Note that binding is commutative and associative. If SDMbind(u, v) == c, then\n",
    "# SDMbind(v, u) == c, and SDMbind(u, c) == v and SDMbind(v, c) == u. Thus given any\n",
    "# two of these vectors you can reconstruct the third.\n",
    "\n",
    "# Also note that the dot in `xor.(u, v)` is Julia's \"broadcast\" notation: xor\n",
    "# is applied elementwise to all pairs of bits in the two vectors.\n",
    "\n",
    "SDMbind(u::BitVector, v::BitVector) = xor.(u, v)\n",
    "\n",
    "SDMbind(ulabel::String, vlabel::String) = xor.(IDX[ulabel], IDX[vlabel])\n",
    "\n",
    "SDMshowbinding(ulabel::String, vlabel::String) = IDXlookup(SDMfetch(SDMbind(ulabel, vlabel)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Herewith the sequence of commands for constructing the parallelogram\n",
    "# of butterfly/caterpillar terms shown in the essay text. This cell is\n",
    "# marked as a \"raw\" cell so that it won't be executed during initialization;\n",
    "# set it to type \"code\" in order to evaluate.\n",
    "\n",
    "\n",
    "SDMstore(\"butterfly\")\n",
    "\n",
    "SDMstore(\"papillon\")\n",
    "\n",
    "SDMstore(\"EN-FR\", SDMbind(\"butterfly\", \"papillon\"))\n",
    "\n",
    "SDMstore(\"caterpillar\")\n",
    "\n",
    "SDMstore(\"adult-juvenile\", SDMbind(\"butterfly\", \"caterpillar\"))\n",
    "\n",
    "SDMstore(\"chenille\", SDMbind(\"EN-FR\", \"caterpillar\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUNDLING\n",
    "\n",
    "# A second key operation in Kanerva's algebra of high-dimensional vectors\n",
    "# is \"bundling\" (also called \"merging\"). It is a componentwise sum or average,\n",
    "# defined by a majority-rule algorithm.\n",
    "\n",
    "# The bundling of just two vectors is particularly simple: At each bit position i, if\n",
    "# u[i] and v[i] have the same value, then we choose that value in the result vector. \n",
    "# If u[i] and v[i] differ, the result is chosen at random.\n",
    "\n",
    "function SDMbundle(u::BitVector, v::BitVector)\n",
    "    (length(u) == length(v)) || throw(DimensionMismatch(\"Vectors u and v must have the same length.\"))\n",
    "    result = BitVector(d)\n",
    "    for i in 1:d\n",
    "        result[i] = (u[i] == v[i]) ? u[i] : rand(Bool)\n",
    "    end\n",
    "    result\n",
    "end\n",
    "\n",
    "\n",
    "# For bundling more than two vectors, we could simply apply the method above\n",
    "# repeatedly, perhaps in a binary-tree arrangement. But there's a better way -- \n",
    "# better both for speed and for minimizing information loss. We take any number of \n",
    "# input vectors \"v...\" or \"v::VarArg\", then do a signed count at each bit position,\n",
    "# +1 for a 1 bit and -1 for a 0 bit. The output vector has a 1 (or true) wherever \n",
    "# the sum is positive, a 0 (or false) wherever the sum is negative, and a random \n",
    "# bit where the sum is 0.\n",
    "\n",
    "function SDMbundle(v::Vararg{BitVector})\n",
    "    accum = zeros(Int64, d)\n",
    "    result = BitVector(d)\n",
    "    for vec in v\n",
    "        for i in 1:d\n",
    "            accum[i] += vec[i] ? 1 : -1\n",
    "        end\n",
    "    end\n",
    "    for j in 1:d\n",
    "        if accum[j] > 0\n",
    "            result[j] = true\n",
    "        elseif accum[j] < 0\n",
    "            result[j] = false\n",
    "        else\n",
    "            result[j] = rand(Bool)\n",
    "        end\n",
    "    end\n",
    "    result\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
